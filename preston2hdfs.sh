#!/bin/bash
#
# Imports Preston data from https://deeplinker.bio into HDFS at hdfs://guoda/data/source=preston/data (for data) and hdfs://guoda/data/source=preston/prov (for data provenance).
#
# 
#

set -x

PRESTON_VERSION=0.0.15
#PRESTON_REMOTE=https://deeplinker.bio
PRESTON_REMOTE=https://raw.githubusercontent.com/bio-guoda/preston-amazon/master/data/
HDFS_TARGET=/$USER/guoda/data/source=preston-amazon
WORK_DIR=preston2hdfs.tmp

echo preston remote [$PRESTON_REMOTE]
echo hdfs target [$HDFS_TARGET]

mkdir -p $WORK_DIR
cd $WORK_DIR

wget https://github.com/bio-guoda/preston/releases/download/${PRESTON_VERSION}/preston.jar -O preston.jar

# list available dataset provenance graph, using remote if needed.
java -jar preston.jar history --remote $PRESTON_REMOTE --log tsv --no-cache | tr '\t' '\n' | grep "hash:\/\/sha256" | sort | uniq > prov_hashes.tsv

java -jar preston.jar ls --remote $PRESTON_REMOTE --log tsv --no-cache | tr '\t' '\n' | grep "hash:\/\/sha256" | sort | uniq > data_hashes.tsv

hdfs dfs -mkdir -p $HDFS_TARGET/prov
hdfs dfs -mkdir -p $HDFS_TARGET/data

# generate scripts to push preston tracked content into hdfs without local caching 
append_upload_script() {
  cat $1 | awk -v DIR_NAME=$2 -v PRESTON_REMOTE=$PRESTON_REMOTE -v HDFS_TARGET=$HDFS_TARGET '{ print "echo " $1 " | java -jar preston.jar get --remote " PRESTON_REMOTE " --no-cache | hdfs dfs -put -p - " HDFS_TARGET "/" DIR_NAME "/" substr($1, 15, 2) "/" substr($1, 17, 2) "/" substr($1, 15, 65) }' >> upload.sh
}

echo "# upload script generated by $0" > upload.sh
append_upload_script data_hashes.tsv data 
append_upload_script prov_hashes.tsv prov

split upload.sh -nl/3 --additional-suffix=.sh -d upload
nohup bash upload00.sh &> upload00.log &
nohup bash upload01.sh &> upload01.log &
nohup bash upload02.sh &> upload02.log &
